{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-14T22:00:18.798547Z",
     "start_time": "2025-01-14T22:00:17.302826Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q datasets langchain langchain-core langchain-community langchain-aws GitPython langchain-text-splitters langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['repo',\n 'instance_id',\n 'base_commit',\n 'patch',\n 'test_patch',\n 'problem_statement',\n 'hints_text',\n 'created_at',\n 'version',\n 'FAIL_TO_PASS',\n 'PASS_TO_PASS',\n 'environment_setup_commit']"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the swe-bench lite dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"princeton-nlp/SWE-bench_Lite\")\n",
    "example_row = dataset[\"test\"][0]\n",
    "list(example_row.keys())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T22:00:20.676711Z",
     "start_time": "2025-01-14T22:00:18.799950Z"
    }
   },
   "id": "283652e5149be132",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff --git a/astropy/modeling/separable.py b/astropy/modeling/separable.py\n",
      "--- a/astropy/modeling/separable.py\n",
      "+++ b/astropy/modeling/separable.py\n",
      "@@ -242,7 +242,7 @@ def _cstack(left, right):\n",
      "         cright = _coord_matrix(right, 'right', noutp)\n",
      "     else:\n",
      "         cright = np.zeros((noutp, right.shape[1]))\n",
      "-        cright[-right.shape[0]:, -right.shape[1]:] = 1\n",
      "+        cright[-right.shape[0]:, -right.shape[1]:] = right\n",
      " \n",
      "     return np.hstack([cleft, cright])\n",
      " \n"
     ]
    }
   ],
   "source": [
    "rows = dataset[\"test\"].skip(0)\n",
    "print(rows[0][\"patch\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T22:00:20.682872Z",
     "start_time": "2025-01-14T22:00:20.677600Z"
    }
   },
   "id": "a93acf6967b6f8cf",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff --git a/lib/mpl_toolkits/mplot3d/axes3d.py b/lib/mpl_toolkits/mplot3d/axes3d.py\n",
      "--- a/lib/mpl_toolkits/mplot3d/axes3d.py\n",
      "+++ b/lib/mpl_toolkits/mplot3d/axes3d.py\n",
      "@@ -2128,6 +2128,14 @@ class Axes3D(Axes):\n",
      "         return super().get_tightbbox(renderer, call_axes_locator=call_axes_locator,\n",
      "                                    bbox_extra_artists=bbox_extra_artists)\n",
      " \n",
      "+    def set_visible(self, visible):\n",
      "+        \"\"\"\n",
      "+        Set the axis visibility.\n",
      "+        \"\"\"\n",
      "+        super().set_visible(visible)\n",
      "+        # Update visibility of all 3D artists\n",
      "+        for artist in self._get_axis_list():\n",
      "+            artist.set_visible(visible)\n"
     ]
    }
   ],
   "source": [
    "pred = {\"model_name_or_path\": \"issue-review-local\", \"instance_id\": \"matplotlib__matplotlib-23314\", \"model_patch\": \"diff --git a/lib/mpl_toolkits/mplot3d/axes3d.py b/lib/mpl_toolkits/mplot3d/axes3d.py\\n--- a/lib/mpl_toolkits/mplot3d/axes3d.py\\n+++ b/lib/mpl_toolkits/mplot3d/axes3d.py\\n@@ -2128,6 +2128,14 @@ class Axes3D(Axes):\\n         return super().get_tightbbox(renderer, call_axes_locator=call_axes_locator,\\n                                    bbox_extra_artists=bbox_extra_artists)\\n \\n+    def set_visible(self, visible):\\n+        \\\"\\\"\\\"\\n+        Set the axis visibility.\\n+        \\\"\\\"\\\"\\n+        super().set_visible(visible)\\n+        # Update visibility of all 3D artists\\n+        for artist in self._get_axis_list():\\n+            artist.set_visible(visible)\"}\n",
    "\n",
    "print(pred[\"model_patch\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T22:00:20.686592Z",
     "start_time": "2025-01-14T22:00:20.684675Z"
    }
   },
   "id": "8663e29dd724815",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# repo -> documents\n",
    "import os\n",
    "\n",
    "FILE_EXTENSIONS = [\".py\"]\n",
    "\n",
    "# configure root folder for each repo\n",
    "repo_to_top_folder = {\n",
    "    \"django/django\": \"django\",\n",
    "    \"sphinx-doc/sphinx\": \"sphinx\",\n",
    "    \"scikit-learn/scikit-learn\": \"scikit-learn\",\n",
    "    \"sympy/sympy\": \"sympy\",\n",
    "    \"pytest-dev/pytest\": \"pytest\",\n",
    "    \"matplotlib/matplotlib\": \"matplotlib\",\n",
    "    \"astropy/astropy\": \"astropy\",\n",
    "    \"pydata/xarray\": \"xarray\",\n",
    "    \"mwaskom/seaborn\": \"seaborn\",\n",
    "    \"psf/requests\": \"requests\",\n",
    "    \"pylint-dev/pylint\": \"pylint\",\n",
    "    \"pallets/flask\": \"flask\",\n",
    "}\n",
    "\n",
    "repo_path = f\".playground/{example_row['repo'].split('/')[-1]}\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T22:00:20.690450Z",
     "start_time": "2025-01-14T22:00:20.687263Z"
    }
   },
   "id": "10dd9721644b43f3",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "889"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from git import Repo\n",
    "from git.exc import GitCommandError\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "from langchain_core.documents import Document\n",
    "import hashlib\n",
    "\n",
    "\n",
    "def hash_text(text: str):\n",
    "    return hashlib.md5(text.encode()).hexdigest()\n",
    "\n",
    "\n",
    "def clone_and_get_contents(\n",
    "    clone_url: str, \n",
    "    repo_path: str, \n",
    "    instance_id: str,\n",
    "    target_folder: Optional[str] = None,\n",
    "    checkout: Optional[str] = None,\n",
    "    file_extensions: Optional[List[str]] = None,\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Clone a repository, checkout a specific commit, and return contents of files from a target folder.\n",
    "    \n",
    "    Args:\n",
    "        repo_url: URL of the git repository\n",
    "        commit_hash: Commit hash to checkout\n",
    "        target_folder: Folder within the repository to traverse\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping file paths to their contents\n",
    "    \"\"\"\n",
    "    target_folder = target_folder or \"\"\n",
    "    documents = []\n",
    "\n",
    "    try:\n",
    "        if os.path.isdir(os.path.join(repo_path, \".git\")):\n",
    "            repo = Repo(repo_path)\n",
    "            # If the existing repository is not the same as the one we're trying to\n",
    "            # clone, raise an error.\n",
    "            if repo.remotes.origin.url != clone_url:\n",
    "                raise ValueError(\n",
    "                    \"A different repository is already cloned at this path.\"\n",
    "                )\n",
    "        else:\n",
    "            repo = Repo.clone_from(clone_url, repo_path)\n",
    "        if checkout:\n",
    "            repo.git.checkout(checkout)\n",
    "        target_path = os.path.join(repo_path, target_folder)\n",
    "\n",
    "        # ensure that the target folder exists\n",
    "        if not os.path.exists(target_path):\n",
    "            raise Exception(f\"Target folder '{target_folder}' not found in repository\")\n",
    "\n",
    "        # Collect all file contents\n",
    "        for root, _, files in os.walk(target_path):\n",
    "            for file in files:\n",
    "                # Get path relative to target folder\n",
    "                full_path = os.path.join(root, file)\n",
    "                rel_path = os.path.relpath(full_path, target_path)\n",
    "                extension = Path(rel_path).suffix\n",
    "                if file_extensions:\n",
    "                    if extension not in file_extensions:\n",
    "                        continue\n",
    "                try:\n",
    "                    # only get the text files\n",
    "                    with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        page_content = f.read()\n",
    "                        documents.append(\n",
    "                            Document(\n",
    "                                page_content=page_content, \n",
    "                                metadata={\n",
    "                                    \"file_path\": os.path.join(target_folder, rel_path), \n",
    "                                    \"file_type\": extension,\n",
    "                                    \"instance_ids\": [instance_id],\n",
    "                                }\n",
    "                            )\n",
    "                        )\n",
    "                except Exception as e:\n",
    "                    # non-text files are skipped\n",
    "                    continue\n",
    "\n",
    "        return documents\n",
    "\n",
    "    except GitCommandError as e:\n",
    "        print(f\"Git error occurred: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        return []\n",
    "    \n",
    "docs = clone_and_get_contents(\n",
    "    clone_url=f\"https://github.com/{example_row['repo']}\", \n",
    "    repo_path=repo_path, \n",
    "    instance_id=example_row[\"instance_id\"],\n",
    "    checkout=example_row[\"base_commit\"], \n",
    "    target_folder=repo_to_top_folder[example_row[\"repo\"]],\n",
    "    file_extensions=FILE_EXTENSIONS,\n",
    ")\n",
    "len(docs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T22:00:21.666157Z",
     "start_time": "2025-01-14T22:00:20.691298Z"
    }
   },
   "id": "810c3f88807e425a",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "2441"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "extension_to_language = {\n",
    "    \".py\": \"python\",\n",
    "    \".java\": \"java\", # etc\n",
    "}\n",
    "\n",
    "# split docs by language\n",
    "docs_by_language = {}\n",
    "for doc in docs:\n",
    "    doc_language = extension_to_language[doc.metadata[\"file_type\"]]\n",
    "    if doc_language not in docs_by_language:\n",
    "        docs_by_language[doc_language] = [doc]\n",
    "    else:\n",
    "        docs_by_language[doc_language].append(doc)\n",
    "\n",
    "# splitter\n",
    "split_docs = []\n",
    "for language in docs_by_language:\n",
    "    language_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language=language, \n",
    "        chunk_size=8192, \n",
    "        chunk_overlap=256,\n",
    "    )\n",
    "    new_docs = language_splitter.split_documents(docs_by_language[language])\n",
    "    for doc in new_docs:\n",
    "        # use the file hash as the id\n",
    "        doc.id = hash_text(doc.page_content)\n",
    "    split_docs += new_docs\n",
    "    \n",
    "len(split_docs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T22:00:21.762095Z",
     "start_time": "2025-01-14T22:00:21.667253Z"
    }
   },
   "id": "63f6938264e0b277",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrockConverse, BedrockEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import InMemoryVectorStore, FAISS, Chroma\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.docstore import InMemoryDocstore\n",
    "from botocore.config import Config\n",
    "import faiss\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "bedrock_config = Config(\n",
    "    retries={\n",
    "        \"max_attempts\": 100\n",
    "    }\n",
    ")\n",
    "\n",
    "credentials_profile = os.getenv(\"AWS_PROFILE\", \"ediewald+1111-Admin\")  # change for your AWS_PROFILE\n",
    "\n",
    "embeddings = BedrockEmbeddings(\n",
    "    model_id=\"amazon.titan-embed-text-v2:0\",\n",
    "    credentials_profile_name=credentials_profile,\n",
    "    model_kwargs={\"dimensions\": 256},\n",
    "    config=bedrock_config,\n",
    ")\n",
    "\n",
    "llm_light = ChatBedrockConverse(\n",
    "    model=\"us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "    credentials_profile_name=credentials_profile,\n",
    "    config=bedrock_config,\n",
    ")\n",
    "\n",
    "llm_heavy = ChatBedrockConverse(\n",
    "    model=\"us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    "    credentials_profile_name=credentials_profile,\n",
    "    config=bedrock_config,\n",
    ")\n",
    "\n",
    "# vectorstores per repo so that filters are most efficient\n",
    "vectorstore_base_path = \".vectorstores/\"\n",
    "index_name = example_row[\"repo\"].split(\"/\")[-1]\n",
    "vectorstore_path = os.path.join(vectorstore_base_path, index_name)\n",
    "# vectorstore = Chroma(\n",
    "#     collection_name=example_row[\"repo\"].replace(\"/\", \"-\"),\n",
    "#     embedding_function=embeddings,\n",
    "#     persist_directory=vectorstore_path,\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T22:00:23.642056Z",
     "start_time": "2025-01-14T22:00:21.762952Z"
    }
   },
   "id": "50c3e7f6cdb9771c",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if os.path.exists(vectorstore_path):\n",
    "    vectorstore = FAISS.load_local(vectorstore_path, embeddings=embeddings, index_name=index_name, allow_dangerous_deserialization=True)\n",
    "else:\n",
    "    index = faiss.IndexFlatL2(len(embeddings.embed_query(\"foo\")))\n",
    "    vectorstore = FAISS(\n",
    "        embedding_function=embeddings,\n",
    "        index=index,\n",
    "        index_to_docstore_id={},\n",
    "        docstore=InMemoryDocstore(),\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T22:00:23.943287Z",
     "start_time": "2025-01-14T22:00:23.642966Z"
    }
   },
   "id": "4b13dc7b22c688f6",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ValidationException",
     "evalue": "An error occurred (ValidationException) when calling the InvokeModel operation: Malformed input request: #: required key [input_type] not found#: extraneous key [embed_type] is not permitted#/texts/0: expected maxLength: 2048, actual: 5074#/texts/2: expected maxLength: 2048, actual: 7711#/texts/3: expected maxLength: 2048, actual: 4234#/texts/4: expected maxLength: 2048, actual: 8168#/texts/5: expected maxLength: 2048, actual: 5527#/texts/8: expected maxLength: 2048, actual: 7914#/texts/10: expected maxLength: 2048, actual: 2440#/texts/11: expected maxLength: 2048, actual: 3940#/texts/12: expected maxLength: 2048, actual: 2194#/texts/13: expected maxLength: 2048, actual: 7961#/texts/14: expected maxLength: 2048, actual: 7204#/texts/15: expected maxLength: 2048, actual: 5960#/texts/16: expected maxLength: 2048, actual: 5715#/texts/17: expected maxLength: 2048, actual: 3747#/texts/18: expected maxLength: 2048, actual: 7665#/texts/19: expected maxLength: 2048, actual: 4855#/texts/23: expected maxLength: 2048, actual: 6106#/texts/24: expected maxLength: 2048, actual: 6780#/texts/25: expected maxLength: 2048, actual: 6066#/texts/26: expected maxLength: 2048, actual: 7391#/texts/28: expected maxLength: 2048, actual: 7687#/texts/29: expected maxLength: 2048, actual: 3638#/texts/31: expected maxLength: 2048, actual: 5248#/texts/32: expected maxLength: 2048, actual: 2218#/texts/33: expected maxLength: 2048, actual: 7002#/texts/34: expected maxLength: 2048, actual: 3898#/texts/35: expected maxLength: 2048, actual: 2298#/texts/36: expected maxLength: 2048, actual: 7363#/texts/38: expected maxLength: 2048, actual: 2298#/texts/41: expected maxLength: 2048, actual: 7947#/texts/42: expected maxLength: 2048, actual: 8135#/texts/43: expected maxLength: 2048, actual: 8032#/texts/44: expected maxLength: 2048, actual: 8171#/texts/45: expected maxLength: 2048, actual: 7888#/texts/46: expected maxLength: 2048, actual: 2559#/texts/47: expected maxLength: 2048, actual: 5760#/texts/48: expected maxLength: 2048, actual: 4560#/texts/49: expected maxLength: 2048, actual: 7821#/texts/50: expected maxLength: 2048, actual: 4405#/texts/51: expected maxLength: 2048, actual: 4609#/texts/52: expected maxLength: 2048, actual: 6784#/texts/55: expected maxLength: 2048, actual: 7992#/texts/56: expected maxLength: 2048, actual: 8058#/texts/57: expected maxLength: 2048, actual: 8093#/texts/58: expected maxLength: 2048, actual: 5933#/texts/60: expected maxLength: 2048, actual: 4065#/texts/61: expected maxLength: 2048, actual: 4731#/texts/62: expected maxLength: 2048, actual: 4817#/texts/64: expected maxLength: 2048, actual: 8179#/texts/66: expected maxLength: 2048, actual: 7666#/texts/67: expected maxLength: 2048, actual: 5675#/texts/68: expected maxLength: 2048, actual: 7293#/texts/69: expected maxLength: 2048, actual: 3352#/texts/70: expected maxLength: 2048, actual: 7816#/texts/72: expected maxLength: 2048, actual: 7990#/texts/75: expected maxLength: 2048, actual: 2877#/texts/76: expected maxLength: 2048, actual: 3584#/texts/77: expected maxLength: 2048, actual: 5289#/texts/78: expected maxLength: 2048, actual: 7561#/texts/79: expected maxLength: 2048, actual: 7987#/texts/80: expected maxLength: 2048, actual: 2127#/texts/82: expected maxLength: 2048, actual: 8088#/texts/83: expected maxLength: 2048, actual: 8123#/texts/84: expected maxLength: 2048, actual: 8104#/texts/85: expected maxLength: 2048, actual: 8022#/texts/87: expected maxLength: 2048, actual: 5622#/texts/88: expected maxLength: 2048, actual: 5202#/texts/90: expected maxLength: 2048, actual: 4778#/texts/91: expected maxLength: 2048, actual: 7549#/texts/93: expected maxLength: 2048, actual: 4170#/texts/95: expected maxLength: 2048, actual: 7382, please reformat your input and try again.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValidationException\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 24\u001B[0m\n\u001B[1;32m     21\u001B[0m         vectors \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m bedrock\u001B[38;5;241m.\u001B[39minvoke_model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mrequest_body)[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbody\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mread()[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124membeddings\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m vectors\n\u001B[0;32m---> 24\u001B[0m vecs \u001B[38;5;241m=\u001B[39m \u001B[43membed_cohere\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mdoc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpage_content\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msplit_docs\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msearch_document\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[10], line 21\u001B[0m, in \u001B[0;36membed_cohere\u001B[0;34m(texts, embed_type)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m chunks:\n\u001B[1;32m     14\u001B[0m     request_body \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     15\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodelId\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcohere.embed-multilingual-v3\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     16\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontentType\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapplication/json\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     17\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccept\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapplication/json\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     18\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbody\u001B[39m\u001B[38;5;124m\"\u001B[39m: json\u001B[38;5;241m.\u001B[39mdumps({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtexts\u001B[39m\u001B[38;5;124m\"\u001B[39m: chunk, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124membed_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: embed_type}),\n\u001B[1;32m     19\u001B[0m     }\n\u001B[0;32m---> 21\u001B[0m     vectors \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mbedrock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mrequest_body\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbody\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mread()[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124membeddings\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m vectors\n",
      "File \u001B[0;32m~/code/fanduel/issue-review-experiments/venv/lib/python3.11/site-packages/botocore/client.py:569\u001B[0m, in \u001B[0;36mClientCreator._create_api_method.<locals>._api_call\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    565\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    566\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpy_operation_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m() only accepts keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    567\u001B[0m     )\n\u001B[1;32m    568\u001B[0m \u001B[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001B[39;00m\n\u001B[0;32m--> 569\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_api_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43moperation_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/code/fanduel/issue-review-experiments/venv/lib/python3.11/site-packages/botocore/client.py:1023\u001B[0m, in \u001B[0;36mBaseClient._make_api_call\u001B[0;34m(self, operation_name, api_params)\u001B[0m\n\u001B[1;32m   1019\u001B[0m     error_code \u001B[38;5;241m=\u001B[39m error_info\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mQueryErrorCode\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m error_info\u001B[38;5;241m.\u001B[39mget(\n\u001B[1;32m   1020\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCode\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1021\u001B[0m     )\n\u001B[1;32m   1022\u001B[0m     error_class \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexceptions\u001B[38;5;241m.\u001B[39mfrom_code(error_code)\n\u001B[0;32m-> 1023\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error_class(parsed_response, operation_name)\n\u001B[1;32m   1024\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1025\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parsed_response\n",
      "\u001B[0;31mValidationException\u001B[0m: An error occurred (ValidationException) when calling the InvokeModel operation: Malformed input request: #: required key [input_type] not found#: extraneous key [embed_type] is not permitted#/texts/0: expected maxLength: 2048, actual: 5074#/texts/2: expected maxLength: 2048, actual: 7711#/texts/3: expected maxLength: 2048, actual: 4234#/texts/4: expected maxLength: 2048, actual: 8168#/texts/5: expected maxLength: 2048, actual: 5527#/texts/8: expected maxLength: 2048, actual: 7914#/texts/10: expected maxLength: 2048, actual: 2440#/texts/11: expected maxLength: 2048, actual: 3940#/texts/12: expected maxLength: 2048, actual: 2194#/texts/13: expected maxLength: 2048, actual: 7961#/texts/14: expected maxLength: 2048, actual: 7204#/texts/15: expected maxLength: 2048, actual: 5960#/texts/16: expected maxLength: 2048, actual: 5715#/texts/17: expected maxLength: 2048, actual: 3747#/texts/18: expected maxLength: 2048, actual: 7665#/texts/19: expected maxLength: 2048, actual: 4855#/texts/23: expected maxLength: 2048, actual: 6106#/texts/24: expected maxLength: 2048, actual: 6780#/texts/25: expected maxLength: 2048, actual: 6066#/texts/26: expected maxLength: 2048, actual: 7391#/texts/28: expected maxLength: 2048, actual: 7687#/texts/29: expected maxLength: 2048, actual: 3638#/texts/31: expected maxLength: 2048, actual: 5248#/texts/32: expected maxLength: 2048, actual: 2218#/texts/33: expected maxLength: 2048, actual: 7002#/texts/34: expected maxLength: 2048, actual: 3898#/texts/35: expected maxLength: 2048, actual: 2298#/texts/36: expected maxLength: 2048, actual: 7363#/texts/38: expected maxLength: 2048, actual: 2298#/texts/41: expected maxLength: 2048, actual: 7947#/texts/42: expected maxLength: 2048, actual: 8135#/texts/43: expected maxLength: 2048, actual: 8032#/texts/44: expected maxLength: 2048, actual: 8171#/texts/45: expected maxLength: 2048, actual: 7888#/texts/46: expected maxLength: 2048, actual: 2559#/texts/47: expected maxLength: 2048, actual: 5760#/texts/48: expected maxLength: 2048, actual: 4560#/texts/49: expected maxLength: 2048, actual: 7821#/texts/50: expected maxLength: 2048, actual: 4405#/texts/51: expected maxLength: 2048, actual: 4609#/texts/52: expected maxLength: 2048, actual: 6784#/texts/55: expected maxLength: 2048, actual: 7992#/texts/56: expected maxLength: 2048, actual: 8058#/texts/57: expected maxLength: 2048, actual: 8093#/texts/58: expected maxLength: 2048, actual: 5933#/texts/60: expected maxLength: 2048, actual: 4065#/texts/61: expected maxLength: 2048, actual: 4731#/texts/62: expected maxLength: 2048, actual: 4817#/texts/64: expected maxLength: 2048, actual: 8179#/texts/66: expected maxLength: 2048, actual: 7666#/texts/67: expected maxLength: 2048, actual: 5675#/texts/68: expected maxLength: 2048, actual: 7293#/texts/69: expected maxLength: 2048, actual: 3352#/texts/70: expected maxLength: 2048, actual: 7816#/texts/72: expected maxLength: 2048, actual: 7990#/texts/75: expected maxLength: 2048, actual: 2877#/texts/76: expected maxLength: 2048, actual: 3584#/texts/77: expected maxLength: 2048, actual: 5289#/texts/78: expected maxLength: 2048, actual: 7561#/texts/79: expected maxLength: 2048, actual: 7987#/texts/80: expected maxLength: 2048, actual: 2127#/texts/82: expected maxLength: 2048, actual: 8088#/texts/83: expected maxLength: 2048, actual: 8123#/texts/84: expected maxLength: 2048, actual: 8104#/texts/85: expected maxLength: 2048, actual: 8022#/texts/87: expected maxLength: 2048, actual: 5622#/texts/88: expected maxLength: 2048, actual: 5202#/texts/90: expected maxLength: 2048, actual: 4778#/texts/91: expected maxLength: 2048, actual: 7549#/texts/93: expected maxLength: 2048, actual: 4170#/texts/95: expected maxLength: 2048, actual: 7382, please reformat your input and try again."
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "import os\n",
    "\n",
    "os.environ[\"AWS_PROFILE\"] = credentials_profile\n",
    "\n",
    "def embed_cohere(texts: List[str], embed_type: Literal[\"search_document\", \"search_query\"]) -> List[List[float]]:\n",
    "\n",
    "    bedrock = boto3.client(\"bedrock-runtime\")\n",
    "    # cohere can take 96 texts per request\n",
    "    chunks = [texts[i:i + 96] for i in range(0, len(texts), 96)]\n",
    "    vectors = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        request_body = {\n",
    "            \"modelId\": \"cohere.embed-multilingual-v3\",\n",
    "            \"contentType\": \"application/json\",\n",
    "            \"accept\": \"application/json\",\n",
    "            \"body\": json.dumps({\"texts\": chunk, \"embed_type\": embed_type}),\n",
    "        }\n",
    "\n",
    "        vectors += bedrock.invoke_model(**request_body)[\"body\"].read()[\"embeddings\"]\n",
    "    return vectors\n",
    "\n",
    "vecs = embed_cohere([doc.page_content for doc in split_docs], \"search_document\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T22:00:25.294729Z",
     "start_time": "2025-01-14T22:00:23.945827Z"
    }
   },
   "id": "6925415bcdea0215",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# parallel embeddings\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Any\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def parallel_embed(texts: List[str], embeddings, max_workers: int = 4) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Parallelize the embedding of texts using a configurable number of workers.\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str]): List of texts to embed\n",
    "        max_workers (int): Maximum number of parallel workers (default: 4)\n",
    "        \n",
    "    Returns:\n",
    "        List[Any]: List of embeddings in the same order as input texts\n",
    "    \"\"\"\n",
    "    # Dictionary to keep track of the original order\n",
    "    results_dict = {}\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks and store futures with their indices\n",
    "        future_to_idx = {\n",
    "            executor.submit(embeddings._embedding_func, text): idx\n",
    "            for idx, text in enumerate(texts)\n",
    "        }\n",
    "\n",
    "        # Process completed futures and store results\n",
    "        for future in as_completed(future_to_idx):\n",
    "            idx = future_to_idx[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results_dict[idx] = result\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing text at index {idx}: {str(e)}\")\n",
    "                results_dict[idx] = None\n",
    "\n",
    "    # Return results in original order\n",
    "    return [results_dict[i] for i in range(len(texts))]\n",
    "\n",
    "\n",
    "def add_docs_to_store(vectorstore: FAISS, embeddings: BedrockEmbeddings, documents: List[Document], max_workers: int = 100):\n",
    "    # find the existing docs, if any\n",
    "    existing_docs = [doc for doc in vectorstore.get_by_ids([d.id for d in documents])]\n",
    "    print(f\"found {len(existing_docs)} existing docs\")\n",
    "    \n",
    "    # only need to embed the new/updated files\n",
    "    new_docs = [doc for doc in documents if doc not in existing_docs]\n",
    "    print(f\"found {len(new_docs)} new docs\")\n",
    "    \n",
    "    assert len(documents) - len(existing_docs) == len(new_docs), \"Num. new docs + num updated docs != num docs\"\n",
    "    \n",
    "    # first, update the existing docs if necessary\n",
    "    for doc in existing_docs:\n",
    "        existing_instance_ids = vectorstore.docstore._dict[doc.id].metadata[\"instance_ids\"]\n",
    "        vectorstore.docstore._dict[doc.id].metadata[\"instance_ids\"] = list(set(existing_instance_ids + doc.metadata[\"instance_ids\"]))\n",
    "    \n",
    "    # now, add the new docs\n",
    "    vectors = parallel_embed([doc.page_content for doc in new_docs], embeddings, max_workers=max_workers)\n",
    "    # add embeddings to FAISS\n",
    "    vectorstore.index.add(np.array(vectors, dtype=np.float32))\n",
    "    # add metadata / docs to docstore\n",
    "    vectorstore.docstore.add({doc.id: doc for doc in new_docs})\n",
    "    # link metadata with FAISS via index_to_docstore_id\n",
    "    starting_len = len(vectorstore.index_to_docstore_id)\n",
    "    index_to_id = {starting_len + j: id_ for j, id_ in enumerate([doc.id for doc in new_docs])}\n",
    "    vectorstore.index_to_docstore_id.update(index_to_id)\n",
    "\n",
    "\n",
    "add_docs_to_store(vectorstore, embeddings, documents=split_docs)\n",
    "# vectorstore.save_local(vectorstore_path, index_name=index_name)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4cd5166cdb23bd56",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vectorstore.similarity_search(\"filter\", filter=lambda x: 'django__django-d' in x[\"instance_ids\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T22:00:25.296664Z",
     "start_time": "2025-01-14T22:00:25.296466Z"
    }
   },
   "id": "95d95d062981625",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(example_row[\"problem_statement\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "103a7f152ef02c93",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'queries': ['separability_matrix',\n  'astropy.modeling.separable',\n  'separability implementation']}"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fault localization\n",
    "from typing import TypedDict, Annotated\n",
    "\n",
    "# file we want: django/core/management/commands/sqlmigrate.py\n",
    "# print(example_row[\"patch\"])\n",
    "\n",
    "class CodeSearch(TypedDict):\n",
    "    \"\"\"The search queries to submit to the code search engine\"\"\"\n",
    "    queries: Annotated[List[str], \"The search queries (can be natural language or exact search)\"]\n",
    "\n",
    "PROMPT_FAULT_LOCALIZATION_SEARCH = \"\"\"You are a senior software engineer working on addressing the following issue:\n",
    "\n",
    "---BEGIN ISSUE---\n",
    "{problem_statement}\n",
    "---END ISSUE---\n",
    "\n",
    "Your first objective is to identify the file(s) that you will need to edit in order to address the issue.\n",
    "\n",
    "You have access to a code search tool that you can use to search through the codebase. Submit 3-5 search queries that you want to use to find the right file(s).\n",
    "\"\"\"\n",
    "\n",
    "llm_search_queries = llm_light.with_structured_output(CodeSearch)\n",
    "\n",
    "res = llm_search_queries.invoke(PROMPT_FAULT_LOCALIZATION_SEARCH.format(problem_statement=example_row[\"problem_statement\"]))\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T22:01:24.204689Z",
     "start_time": "2025-01-14T22:01:22.032521Z"
    }
   },
   "id": "58804753b95cb31d",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m relevant_docs \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m query \u001B[38;5;129;01min\u001B[39;00m res[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mqueries\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m----> 3\u001B[0m     relevant_docs \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mvectorstore\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msimilarity_search_with_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m sorted_docs \u001B[38;5;241m=\u001B[39m [doc \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28msorted\u001B[39m(relevant_docs, key\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m x: x[\u001B[38;5;241m1\u001B[39m], reverse\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)]\n\u001B[1;32m      5\u001B[0m unique_files, file_scores \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/code/fanduel/issue-review-experiments/venv/lib/python3.11/site-packages/langchain_community/vectorstores/faiss.py:516\u001B[0m, in \u001B[0;36mFAISS.similarity_search_with_score\u001B[0;34m(self, query, k, filter, fetch_k, **kwargs)\u001B[0m\n\u001B[1;32m    499\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Return docs most similar to query.\u001B[39;00m\n\u001B[1;32m    500\u001B[0m \n\u001B[1;32m    501\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    513\u001B[0m \u001B[38;5;124;03m    L2 distance in float. Lower score represents more similarity.\u001B[39;00m\n\u001B[1;32m    514\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    515\u001B[0m embedding \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_embed_query(query)\n\u001B[0;32m--> 516\u001B[0m docs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msimilarity_search_with_score_by_vector\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    517\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    518\u001B[0m \u001B[43m    \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    519\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mfilter\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mfilter\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    520\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfetch_k\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfetch_k\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    521\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    522\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m docs\n",
      "File \u001B[0;32m~/code/fanduel/issue-review-experiments/venv/lib/python3.11/site-packages/langchain_community/vectorstores/faiss.py:417\u001B[0m, in \u001B[0;36mFAISS.similarity_search_with_score_by_vector\u001B[0;34m(self, embedding, k, filter, fetch_k, **kwargs)\u001B[0m\n\u001B[1;32m    415\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_normalize_L2:\n\u001B[1;32m    416\u001B[0m     faiss\u001B[38;5;241m.\u001B[39mnormalize_L2(vector)\n\u001B[0;32m--> 417\u001B[0m scores, indices \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msearch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvector\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mfilter\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mfetch_k\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    418\u001B[0m docs \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    420\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mfilter\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/code/fanduel/issue-review-experiments/venv/lib/python3.11/site-packages/faiss/class_wrappers.py:329\u001B[0m, in \u001B[0;36mhandle_Index.<locals>.replacement_search\u001B[0;34m(self, x, k, params, D, I)\u001B[0m\n\u001B[1;32m    327\u001B[0m n, d \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mshape\n\u001B[1;32m    328\u001B[0m x \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mascontiguousarray(x, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfloat32\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m--> 329\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m d \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39md\n\u001B[1;32m    331\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m k \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    333\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m D \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mAssertionError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "relevant_docs = []\n",
    "for query in res[\"queries\"]:\n",
    "    relevant_docs += vectorstore.similarity_search_with_score(query, k=5)\n",
    "sorted_docs = [doc for doc in sorted(relevant_docs, key=lambda x: x[1], reverse=True)]\n",
    "unique_files, file_scores = [], []\n",
    "for doc, score in sorted_docs:\n",
    "    file_path = doc.metadata[\"file_path\"]\n",
    "    if file_path not in unique_files:\n",
    "        unique_files.append(file_path)\n",
    "        file_scores.append(score)\n",
    "\n",
    "# able to identify the right file, but we should assume it will not always be the top result\n",
    "print(\"\\n\".join([f\"{round(score, 2)} | {fp}\" for fp, score in zip(unique_files, file_scores)]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T22:01:27.664849Z",
     "start_time": "2025-01-14T22:01:26.853115Z"
    }
   },
   "id": "c0da0bbad701ef90",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unique_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 34\u001B[0m\n\u001B[1;32m     30\u001B[0m         contents \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mprefix\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mline\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m contents\n\u001B[0;32m---> 34\u001B[0m file_candidates \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([get_code_file(repo_path, fp) \u001B[38;5;28;01mfor\u001B[39;00m fp \u001B[38;5;129;01min\u001B[39;00m \u001B[43munique_files\u001B[49m])\n\u001B[1;32m     36\u001B[0m llm_patch \u001B[38;5;241m=\u001B[39m llm_heavy\u001B[38;5;241m.\u001B[39mwith_structured_output(SolutionPatch)\n\u001B[1;32m     38\u001B[0m model_patch \u001B[38;5;241m=\u001B[39m llm_patch\u001B[38;5;241m.\u001B[39minvoke(\n\u001B[1;32m     39\u001B[0m     PROMPT_GENERATE_PATCH\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m     40\u001B[0m         problem_statement\u001B[38;5;241m=\u001B[39mexample_row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mproblem_statement\u001B[39m\u001B[38;5;124m\"\u001B[39m], \n\u001B[1;32m     41\u001B[0m         file_candidates\u001B[38;5;241m=\u001B[39mfile_candidates\n\u001B[1;32m     42\u001B[0m     ),\n\u001B[1;32m     43\u001B[0m )[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpatch\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'unique_files' is not defined"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "\n",
    "class SolutionPatch(TypedDict):\n",
    "    \"\"\"A git patch to address the provided issue.\"\"\"\n",
    "    patch: Annotated[str, \"The git patch.\"]\n",
    "    \n",
    "\n",
    "PROMPT_GENERATE_PATCH = \"\"\"You are a senior software engineer addressing the following issue:\n",
    "\n",
    "---BEGIN ISSUE---\n",
    "{problem_statement}\n",
    "---END ISSUE---\n",
    "\n",
    "We've identified that the fix will likely require edits to one or more of the following file(s) (note that line numbers have been added for reference):\n",
    "\n",
    "---BEGIN CODE FILES---\n",
    "{file_candidates}\n",
    "---END CODE FILES---\n",
    "\n",
    "Instructions:\n",
    "1. Carefully read the problem statement and the provided code files.\n",
    "2. Create a git patch that will address the issue. You may only need to edit one of the files that was provided. Your patch should NOT include an `index` header.\"\"\"\n",
    "\n",
    "def get_code_file(repo_path: str, file_path: str, line_numbers: bool = True) -> str:\n",
    "    contents = f\"File: {file_path}\\n\\n\"\n",
    "    with open(os.path.join(repo_path, file_path), \"r\") as f:\n",
    "        code_lines = f.read()\n",
    "    for idx, line in enumerate(code_lines.splitlines()):\n",
    "        prefix = f\"L{idx+1}: \" if line_numbers else \"\"\n",
    "        contents += f\"{prefix}{line}\\n\"\n",
    "    return contents\n",
    "\n",
    "\n",
    "file_candidates = \"\\n\\n\".join([get_code_file(repo_path, fp) for fp in unique_files])\n",
    "\n",
    "llm_patch = llm_heavy.with_structured_output(SolutionPatch)\n",
    "\n",
    "model_patch = llm_patch.invoke(\n",
    "    PROMPT_GENERATE_PATCH.format(\n",
    "        problem_statement=example_row[\"problem_statement\"], \n",
    "        file_candidates=file_candidates\n",
    "    ),\n",
    ")[\"patch\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T22:01:10.894488Z",
     "start_time": "2025-01-14T22:01:10.882375Z"
    }
   },
   "id": "d38d0c9b4d7eeaee",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file_candidates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mfile_candidates\u001B[49m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'file_candidates' is not defined"
     ]
    }
   ],
   "source": [
    "print(file_candidates)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T22:00:47.220151Z",
     "start_time": "2025-01-14T22:00:47.105052Z"
    }
   },
   "id": "d2557e180e9425f7",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(\"Target Patch:\")\n",
    "print(example_row[\"patch\"])\n",
    "\n",
    "print(\"Model Patch:\")\n",
    "print(model_patch)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d8912a125d688fd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "3503"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_directory_structure(repo_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get the directory structure of a repository.\n",
    "\n",
    "    Args:\n",
    "        repo_path (str): Path to the repository\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of file paths relative to the repository root\n",
    "    \"\"\"\n",
    "    file_paths = []\n",
    "    for root, _, files in os.walk(repo_path):\n",
    "        for file in files:\n",
    "            # Skip hidden files\n",
    "            if file.startswith(\".\"):\n",
    "                continue\n",
    "            # Get the relative path\n",
    "            relative_path = os.path.relpath(os.path.join(root, file), repo_path)\n",
    "            file_paths.append(relative_path)\n",
    "    return file_paths\n",
    "\n",
    "repo_path = \".playground/django/django\"\n",
    "len(get_directory_structure(repo_path))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T22:10:09.898605Z",
     "start_time": "2025-01-14T22:10:09.386904Z"
    }
   },
   "id": "9d2ae9fecb53d0b",
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
