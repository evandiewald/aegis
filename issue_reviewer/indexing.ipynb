{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone repo and checkout locally\n",
    "\n",
    "# load python files from directory\n",
    "\n",
    "# split into documents\n",
    "\n",
    "# embed and store in vectorstore\n",
    "# metadata: name (e.g. MyClass.my_function) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "DATASET_ID = \"princeton-nlp/SWE-bench_Lite\"\n",
    "SPLIT = \"test\"\n",
    "INSTANCE_ID = \"sympy__sympy-20442\"\n",
    "RUN_ID = \"v0\"\n",
    "\n",
    "dataset = load_dataset(DATASET_ID, split=SPLIT)\n",
    "instance_details = [r for r in dataset if r[\"instance_id\"] == INSTANCE_ID][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository already exists at ../.repos/sympy\n",
      "Fetching all remote branches...\n",
      "Successfully checked out commit 1abbc0ac3e552cb184317194e5d5c5b9dd8fb640\n"
     ]
    }
   ],
   "source": [
    "import git\n",
    "import os\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "def clone_and_checkout(instance_details: dict) -> str:\n",
    "    local_path = os.path.join(\"../.repos/\", instance_details[\"repo\"].split(\"/\")[-1])\n",
    "    repo_url = f\"https://github.com/{instance_details['repo']}.git\"\n",
    "    commit_sha = instance_details[\"base_commit\"]\n",
    "    # Clone the repository if it doesn't exist\n",
    "    if not os.path.exists(local_path):\n",
    "        print(f\"Cloning repository from {repo_url}...\")\n",
    "        repo = git.Repo.clone_from(repo_url, local_path)\n",
    "        print(\"Repository cloned successfully!\")\n",
    "    else:\n",
    "        print(f\"Repository already exists at {local_path}\")\n",
    "        repo = git.Repo(local_path)\n",
    "\n",
    "    # Fetch all remote branches\n",
    "    print(\"Fetching all remote branches...\")\n",
    "    repo.remotes.origin.fetch()\n",
    "\n",
    "    # Checkout the specific commit\n",
    "    repo.git.checkout(commit_sha)\n",
    "    print(f\"Successfully checked out commit {commit_sha}\")\n",
    "    return local_path\n",
    "\n",
    "repo_path = clone_and_checkout(instance_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree_sitter import Language, Parser\n",
    "import tree_sitter_python as tspython\n",
    "\n",
    "import os\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, List\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "class CodeBlock(BaseModel):\n",
    "    name: str  # Full path (e.g., \"module.class.function\")\n",
    "    type: str  # \"class\" or \"function\"\n",
    "    code: str  # Complete code block\n",
    "    docstring: Optional[str]\n",
    "    file_path: str\n",
    "    start_line: int\n",
    "    end_line: int\n",
    "    parent: Optional[str]  # Parent class/module name\n",
    "    category: str  # tests / src\n",
    "    id: str  # hash of the code\n",
    "\n",
    "def setup_parser():\n",
    "    \"\"\"Initialize the tree-sitter parser for Python.\"\"\"\n",
    "    PY_LANGUAGE = Language(tspython.language())\n",
    "    parser = Parser(PY_LANGUAGE)\n",
    "    return parser\n",
    "\n",
    "def extract_docstring(node, source_code):\n",
    "    \"\"\"Extract docstring from a class or function node.\"\"\"\n",
    "    for child in node.children:\n",
    "        if child.type == 'block':\n",
    "            for block_child in child.children:\n",
    "                if block_child.type == 'expression_statement':\n",
    "                    for expr_child in block_child.children:\n",
    "                        if expr_child.type == 'string':\n",
    "                            return source_code[expr_child.start_byte:expr_child.end_byte].strip('\\\"\\'')\n",
    "    return None\n",
    "\n",
    "def get_node_source(node, source_code):\n",
    "    \"\"\"Get the source code for a node.\"\"\"\n",
    "    return source_code[node.start_byte:node.end_byte]\n",
    "\n",
    "def process_file(file_path: str, parser: Parser) -> List[CodeBlock]:\n",
    "    \"\"\"Process a single Python file and extract all code blocks.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        source_code = f.read()\n",
    "\n",
    "    tree = parser.parse(bytes(source_code, \"utf8\"))\n",
    "    blocks = []\n",
    "    \n",
    "    def process_node(node, parent_name=None):\n",
    "\n",
    "        code = get_node_source(node, source_code)\n",
    "        id = hashlib.md5(code.encode('utf-8')).hexdigest()\n",
    "        category = \"tests\" if \"test\" in file_path else \"src\"\n",
    "\n",
    "        if node.type == 'class_definition':\n",
    "            name_node = next(child for child in node.children if child.type == 'identifier')\n",
    "            class_name = source_code[name_node.start_byte:name_node.end_byte]\n",
    "            \n",
    "            full_name = f\"{parent_name}.{class_name}\" if parent_name else class_name\n",
    "            \n",
    "            blocks.append(CodeBlock(\n",
    "                name=full_name,\n",
    "                type=\"class\",\n",
    "                code=code,\n",
    "                docstring=extract_docstring(node, source_code),\n",
    "                file_path=file_path,\n",
    "                start_line=node.start_point[0] + 1,\n",
    "                end_line=node.end_point[0] + 1,\n",
    "                parent=parent_name,\n",
    "                category=category,\n",
    "                id=id,\n",
    "            ))\n",
    "            \n",
    "            # Process methods within the class\n",
    "            for child in node.children:\n",
    "                if child.type == 'block':\n",
    "                    for block_child in child.children:\n",
    "                        process_node(block_child, full_name)\n",
    "                        \n",
    "        elif node.type == 'function_definition':\n",
    "            name_node = next(child for child in node.children if child.type == 'identifier')\n",
    "            func_name = source_code[name_node.start_byte:name_node.end_byte]\n",
    "            \n",
    "            full_name = f\"{parent_name}.{func_name}\" if parent_name else func_name\n",
    "            \n",
    "            blocks.append(CodeBlock(\n",
    "                name=full_name,\n",
    "                type=\"function\",\n",
    "                code=code,\n",
    "                docstring=extract_docstring(node, source_code),\n",
    "                file_path=file_path,\n",
    "                start_line=node.start_point[0] + 1,\n",
    "                end_line=node.end_point[0] + 1,\n",
    "                parent=parent_name,\n",
    "                category=category,\n",
    "                id=id,\n",
    "            ))\n",
    "\n",
    "    # Start processing from the root\n",
    "    for node in tree.root_node.children:\n",
    "        process_node(node)\n",
    "        \n",
    "    return blocks\n",
    "\n",
    "def analyze_directory(directory_path: str) -> List[CodeBlock]:\n",
    "    \"\"\"\n",
    "    Analyze all Python files in a directory and extract code blocks.\n",
    "    \n",
    "    Args:\n",
    "        directory_path: Path to the directory to analyze\n",
    "        \n",
    "    Returns:\n",
    "        List of CodeBlock objects containing the extracted information\n",
    "    \"\"\"\n",
    "    parser = setup_parser()\n",
    "    all_blocks = []\n",
    "    \n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.py'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    blocks = process_file(file_path, parser)\n",
    "                    all_blocks.extend(blocks)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {str(e)}\")\n",
    "                    \n",
    "    return all_blocks\n",
    "\n",
    "\n",
    "code_blocks = analyze_directory(repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def prepare_code_blocks(code_blocks: List[CodeBlock]) -> Tuple[List[str], List[str], List[Dict]]:\n",
    "    # texts, ids, metadatas\n",
    "    texts = [\"\\n\".join([c.name, c.type, c.code]) for c in code_blocks]\n",
    "    ids = [hashlib.md5(json.dumps(c.model_dump()).encode(\"utf-8\")).hexdigest() for c in code_blocks]\n",
    "    metadatas = [{\n",
    "        \"file_path\": c.file_path,\n",
    "        \"start_line\": c.start_line,\n",
    "        \"end_line\": c.end_line,\n",
    "        \"category\": c.category,\n",
    "        \"type\": c.type,\n",
    "        \"name\": c.name,\n",
    "    } for c in code_blocks]\n",
    "    return texts, ids, metadatas\n",
    "\n",
    "\n",
    "texts, ids, metadatas = prepare_code_blocks(code_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from typing import List, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "MAX_CHARACTERS_PER_DOC = 2048\n",
    "\n",
    "def embed_cohere(texts: List[str], model_id: str = \"cohere.embed-english-v3\", input_type: str = \"search_query\") -> List[List[float]]:\n",
    "    runtime_client = boto3.client('bedrock-runtime', region_name='us-east-1')\n",
    "        \n",
    "    # `truncate` parameter does not seem to do anything\n",
    "    payload = {\n",
    "        \"texts\": [text[:MAX_CHARACTERS_PER_DOC] for text in texts],\n",
    "        \"input_type\": input_type,\n",
    "        \"truncate\": \"END\"\n",
    "    }\n",
    "    \n",
    "    response = runtime_client.invoke_model(\n",
    "        body=json.dumps(payload),\n",
    "        modelId=model_id,\n",
    "    )\n",
    "    \n",
    "    return json.loads(response['body'].read().decode())[\"embeddings\"]\n",
    "\n",
    "\n",
    "def process_batch(batch_with_index, model_id: str) -> tuple[int, List[List[float]]]:\n",
    "    \"\"\"\n",
    "    Process a single batch of texts and return embeddings with the batch index.\n",
    "    \n",
    "    Args:\n",
    "        batch_with_index (tuple): Tuple of (batch_index, texts)\n",
    "        endpoint_name (str): Name of the SageMaker endpoint\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (batch_index, embeddings)\n",
    "    \"\"\"\n",
    "    batch_index, batch_texts = batch_with_index\n",
    "    try:\n",
    "        batch_embeddings = embed_cohere(batch_texts, model_id, \"search_document\") \n",
    "        return batch_index, batch_embeddings\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {batch_index}: {str(e)}\")\n",
    "        return batch_index, None\n",
    "\n",
    "def get_embeddings_parallel(\n",
    "    texts: List[str],\n",
    "    model_id: str = \"cohere.embed-english-v3\",\n",
    "    batch_size: int = 96,\n",
    "    max_workers: int = 16,\n",
    "    show_progress: bool = True\n",
    ") -> Optional[List[List[float]]]:\n",
    "    \"\"\"\n",
    "    Get embeddings for a list of texts using parallel processing.\n",
    "    \n",
    "    Args:\n",
    "        texts (List[str]): List of strings to get embeddings for\n",
    "        endpoint_name (str): Name of the SageMaker endpoint\n",
    "        batch_size (int): Number of texts to process in each batch\n",
    "        max_workers (int): Maximum number of parallel threads\n",
    "        show_progress (bool): Whether to show progress bar\n",
    "    \n",
    "    Returns:\n",
    "        Optional[List[List[float]]]: List of embeddings in the same order as input texts\n",
    "    \"\"\"\n",
    "    # Create batches with their indices\n",
    "    batches = [\n",
    "        (batch_idx, texts[i:i + batch_size]) \n",
    "        for batch_idx, i in enumerate(range(0, len(texts), batch_size))\n",
    "    ]\n",
    "    \n",
    "    # Initialize results storage\n",
    "    results = {}\n",
    "    all_embeddings = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all batches to the executor\n",
    "        future_to_batch = {\n",
    "            executor.submit(process_batch, batch, model_id): batch[0] \n",
    "            for batch in batches\n",
    "        }\n",
    "        \n",
    "        # Process completed futures with optional progress bar\n",
    "        futures_iterator = as_completed(future_to_batch)\n",
    "        if show_progress:\n",
    "            futures_iterator = tqdm(\n",
    "                futures_iterator, \n",
    "                total=len(batches),\n",
    "                desc=\"Processing batches\"\n",
    "            )\n",
    "        \n",
    "        # Collect results\n",
    "        for future in futures_iterator:\n",
    "            batch_index, batch_embeddings = future.result()\n",
    "            if batch_embeddings is None:\n",
    "                return None\n",
    "            results[batch_index] = batch_embeddings\n",
    "\n",
    "    # Combine results in correct order\n",
    "    for i in range(0, len(batches)):\n",
    "        all_embeddings.extend(results[i])\n",
    "    \n",
    "    return all_embeddings\n",
    "        \n",
    "    \n",
    "embeddings = get_embeddings_parallel(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "\n",
    "db_path = f\"../.vectors/{instance_details['instance_id']}\"\n",
    "\n",
    "\n",
    "db = Chroma(\n",
    "    collection_name=instance_details[\"instance_id\"],\n",
    "    persist_directory=db_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32476"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db._collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "db._collection.add(\n",
    "    ids=ids,\n",
    "    embeddings=embeddings,\n",
    "    metadatas=metadatas,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'category': 'src',\n",
       "   'end_line': 952,\n",
       "   'file_path': '../.repos/sympy/sympy/matrices/sparse.py',\n",
       "   'name': 'MutableSparseMatrix.copyin_matrix',\n",
       "   'start_line': 928,\n",
       "   'type': 'function'},\n",
       "  {'category': 'tests',\n",
       "   'end_line': 181,\n",
       "   'file_path': '../.repos/sympy/sympy/printing/tests/test_mathematica.py',\n",
       "   'name': 'test_matrices',\n",
       "   'start_line': 150,\n",
       "   'type': 'function'},\n",
       "  {'category': 'src',\n",
       "   'end_line': 130,\n",
       "   'file_path': '../.repos/sympy/sympy/matrices/expressions/blockmatrix.py',\n",
       "   'name': 'BlockMatrix.__new__',\n",
       "   'start_line': 79,\n",
       "   'type': 'function'},\n",
       "  {'category': 'src',\n",
       "   'end_line': 188,\n",
       "   'file_path': '../.repos/sympy/sympy/matrices/immutable.py',\n",
       "   'name': 'ImmutableSparseMatrix',\n",
       "   'start_line': 123,\n",
       "   'type': 'class'},\n",
       "  {'category': 'src',\n",
       "   'end_line': 300,\n",
       "   'file_path': '../.repos/sympy/sympy/matrices/sparse.py',\n",
       "   'name': 'SparseMatrix.__getitem__',\n",
       "   'start_line': 258,\n",
       "   'type': 'function'},\n",
       "  {'category': 'src',\n",
       "   'end_line': 116,\n",
       "   'file_path': '../.repos/sympy/sympy/matrices/immutable.py',\n",
       "   'name': 'ImmutableDenseMatrix',\n",
       "   'start_line': 30,\n",
       "   'type': 'class'},\n",
       "  {'category': 'src',\n",
       "   'end_line': 376,\n",
       "   'file_path': '../.repos/sympy/sympy/matrices/common.py',\n",
       "   'name': 'MatrixShaping.extract',\n",
       "   'start_line': 316,\n",
       "   'type': 'function'},\n",
       "  {'category': 'src',\n",
       "   'end_line': 134,\n",
       "   'file_path': '../.repos/sympy/sympy/stats/stochastic_process_types.py',\n",
       "   'name': '_matrix_checks',\n",
       "   'start_line': 126,\n",
       "   'type': 'function'},\n",
       "  {'category': 'tests',\n",
       "   'end_line': 27,\n",
       "   'file_path': '../.repos/sympy/sympy/printing/tests/test_lambdarepr.py',\n",
       "   'name': 'test_matrix',\n",
       "   'start_line': 20,\n",
       "   'type': 'function'},\n",
       "  {'category': 'src',\n",
       "   'end_line': 238,\n",
       "   'file_path': '../.repos/sympy/sympy/stats/matrix_distributions.py',\n",
       "   'name': 'MatrixGammaDistribution',\n",
       "   'start_line': 203,\n",
       "   'type': 'class'}]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"separability matrix\"\n",
    "\n",
    "query_vec = embed_cohere([query])\n",
    "\n",
    "db._collection.query(query_vec)[\"metadatas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
